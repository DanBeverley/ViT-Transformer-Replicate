{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "aba3c4c0-e296-4464-b764-d9334d7720e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The input shape (single 2D Image) : (224, 224, 3)\n",
      "The output shape (flattened into patches) : (196, 768)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torch import nn\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "\n",
    "height = 224        #H\n",
    "width = 224         #W\n",
    "color_channels = 3  #C\n",
    "patch_size = 16     #P\n",
    "number_of_patches = int(height*width/patch_size**2) #N = HW/P**2\n",
    "\n",
    "input_shape = (height , width , color_channels)\n",
    "output_shape = (number_of_patches , patch_size**2*color_channels)\n",
    "\n",
    "print(f\"The input shape (single 2D Image) : {input_shape}\")\n",
    "print(f\"The output shape (flattened into patches) : {output_shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2177dc3f-ee69-45a9-967c-ccbec3cbc578",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The input image has the shape of torch.Size([1, 3, 224, 224]) -> [Batch_size , Color_channels , Height , Width]\n",
      "The image gets converted to 1D sequence of flattened 2d patches with size N x (P^2 x C)\n",
      "Output Shape : torch.Size([1, 196, 768])\n"
     ]
    }
   ],
   "source": [
    "#Create Patch Embedding Layer\n",
    "class PatchEmbedding(nn.Module):\n",
    "    def __init__(self , patch_size = 16 , in_channels = 3 , out_channels = 768):\n",
    "        super(PatchEmbedding , self).__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.patch = nn.Conv2d(in_channels = in_channels , \n",
    "                              out_channels = out_channels , \n",
    "                              kernel_size = patch_size  ,\n",
    "                              stride = patch_size , padding = 0)\n",
    "        #Flatten\n",
    "        self.flatten = nn.Flatten(start_dim = 2 , end_dim = 3)\n",
    "    def forward(self , x):\n",
    "        return self.flatten(self.patch(x)).permute(0,2,1) #-->[batch_size , N , P**2*C]\n",
    "\n",
    "#Testing\n",
    "input_image = torch.Tensor(1 , 3 , 224 , 224)\n",
    "patch = PatchEmbedding(patch_size = 16 , in_channels = 3 , out_channels = 768)\n",
    "print(f'The input image has the shape of {input_image.shape} -> [Batch_size , Color_channels , Height , Width]')\n",
    "print(\"The image gets converted to 1D sequence of flattened 2d patches with size N x (P^2 x C)\")\n",
    "patch = PatchEmbedding(patch_size = 16 , in_channels = 3 , out_channels = 768)\n",
    "patched_img = patch(input_image)\n",
    "print(f'Output Shape : {patched_img.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0b660839-8181-4874-aae2-d22aa50d2ab9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.9788, -0.0973,  0.7067, -0.2775, -0.1617,  0.8443, -1.6049,\n",
      "          -0.5992, -1.5365,  0.4258]]], grad_fn=<SliceBackward0>)\n",
      "Class token shape : torch.Size([1, 1, 768])\n",
      "Embedding patches with class token : torch.Size([1, 197, 768]) -> [Batch_size , num_patches , embedding_size]\n"
     ]
    }
   ],
   "source": [
    "#Create class token\n",
    "batch_size = patched_img.shape[0]\n",
    "embedding_dimension = patched_img.shape[-1]\n",
    "class_token = nn.Parameter(torch.randn(batch_size, 1 , embedding_dimension),requires_grad = True)\n",
    "print(class_token[:,:,:10])\n",
    "print(f'Class token shape : {class_token.shape}')\n",
    "\n",
    "#Add class token to the beginning of patch embedding\n",
    "patch_embedding_with_class_token = torch.cat((class_token , patched_img), dim = 1)\n",
    "print(f\"Embedding patches with class token : {patch_embedding_with_class_token.shape} -> [Batch_size , num_patches , embedding_size]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a865fe64-b23e-4873-bd4a-c5fc360694a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.8642, -0.0402, -0.0373,  ...,  0.6045,  0.9637,  1.0230],\n",
      "         [-0.5235,  0.9575, -1.4920,  ..., -1.0307,  0.4456, -0.6138],\n",
      "         [-0.4988,  0.9267,  0.0124,  ...,  1.2057, -0.6846, -1.5696],\n",
      "         ...,\n",
      "         [-0.1430,  1.2397, -0.8184,  ..., -1.2074, -1.8293,  0.9498],\n",
      "         [ 1.6196, -2.5626,  0.9129,  ...,  0.9080,  0.4245,  1.2222],\n",
      "         [ 1.1143, -2.7331, -1.4910,  ..., -0.9634,  1.6950,  0.3367]]],\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Position embedding shape : torch.Size([1, 197, 768])\n",
      "Position Embedding with class token tensor([[[ 1.1466e-01, -1.3756e-01,  6.6942e-01,  ...,  2.0557e+00,\n",
      "          -3.0662e-02,  1.3205e+00],\n",
      "         [-4.8788e-01,  9.2873e-01, -1.5038e+00,  ...,  5.5978e-01,\n",
      "          -3.0382e-01, -7.6650e-01],\n",
      "         [-4.6325e-01,  8.9795e-01,  5.0689e-04,  ..., -1.0950e+00,\n",
      "          -1.8274e-01,  2.6580e+00],\n",
      "         ...,\n",
      "         [-1.0736e-01,  1.2109e+00, -8.3023e-01,  ..., -1.3148e+00,\n",
      "          -6.5303e-01, -1.0779e-01],\n",
      "         [ 1.6552e+00, -2.5914e+00,  9.0104e-01,  ..., -3.5154e-01,\n",
      "           1.1699e+00, -1.9381e+00],\n",
      "         [ 1.1499e+00, -2.7618e+00, -1.5029e+00,  ...,  4.3629e-01,\n",
      "           1.5535e-01,  9.5422e-01]]], grad_fn=<AddBackward0>)\n",
      "Shape torch.Size([1, 197, 768])\n"
     ]
    }
   ],
   "source": [
    "#According to the paper position embedding has the shape of (N+1)*D\n",
    "position_embedding = nn.Parameter(torch.randn(1 , number_of_patches+1 , embedding_dimension),requires_grad = True)\n",
    "print(position_embedding[:,:,:10])\n",
    "print(f\"Position embedding shape : {position_embedding.shape}\")\n",
    "\n",
    "#Add position embedding to the beginning of patch embedding with class_token\n",
    "class_patch_embedding = position_embedding + patch_embedding_with_class_token\n",
    "print(f'Position Embedding with class token {class_patch_embedding}')\n",
    "print(f'Shape {class_patch_embedding.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "884e2668-d216-4c41-b710-507677ef2eae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape of MSA : torch.Size([1, 197, 768])\n",
      "Output shape of MSA : torch.Size([1, 197, 768])\n"
     ]
    }
   ],
   "source": [
    "#Equation-2 : Multi-head Attention Block\n",
    "class Msa(nn.Module):\n",
    "    def __init__(self , embedding_dimension = 768 , num_heads = 12):\n",
    "        super(Msa , self).__init__()\n",
    "        self.embedding_dimension = embedding_dimension\n",
    "        self.num_heads = num_heads\n",
    "        self.layer_norm = nn.LayerNorm(normalized_shape = embedding_dimension)\n",
    "        self.msa = nn.MultiheadAttention(embed_dim = embedding_dimension , \n",
    "                                        num_heads = num_heads , batch_first = True)\n",
    "    def forward(self , x):\n",
    "        x = self.layer_norm(x)\n",
    "        attn_output , _ = self.msa(query = x , key = x , value = x , need_weights = False)\n",
    "        return attn_output\n",
    "\n",
    "\n",
    "msa = Msa(embedding_dimension = 768 , num_heads = 12)\n",
    "patched_img_with_msa = msa(class_patch_embedding)\n",
    "print(f'Input shape of MSA : {class_patch_embedding.shape}')\n",
    "print(f'Output shape of MSA : {patched_img_with_msa.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3e33a404-e074-43de-adf3-fa1b78681ac0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input for MLP block : torch.Size([1, 197, 768])\n",
      "Output for MLP block : torch.Size([1, 197, 768])\n"
     ]
    }
   ],
   "source": [
    "#Equation-3 : Multi layer Perceptron Block --- Contains two layers wth a GELU non-linearity\n",
    "class MLPBlock(nn.Module):\n",
    "    def __init__(self, embedding_dimension = 768 , mlp_size = 3072):\n",
    "        super(MLPBlock , self).__init__()\n",
    "        self.embedding_dimension = embedding_dimension\n",
    "        self.mlp_size = mlp_size\n",
    "        self.layer_norm = nn.LayerNorm(normalized_shape = embedding_dimension)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(in_features = 768 , out_features = 3072),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(in_features = 3072 , out_features = embedding_dimension)\n",
    "        )\n",
    "    def forward(self , x):\n",
    "        return self.mlp(self.layer_norm(x))\n",
    "\n",
    "\n",
    "mlp = MLPBlock(embedding_dimension = 768 , mlp_size = 3072)\n",
    "patched_image_through_mlp = mlp(patched_img_with_msa)\n",
    "print(f'Input for MLP block : {patched_img_with_msa.shape}')\n",
    "print(f'Output for MLP block : {patched_image_through_mlp.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8eb6ce01-b643-4a7d-959d-4d396e532b1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create transformer encoder\n",
    "class TransformerEncoder(nn.Module):\n",
    "    def __init__(self , embedding_dimension = 768 , num_heads = 12 , mlp_size = 3072):\n",
    "        super(TransformerEncoder , self).__init__()\n",
    "        self.embedding_dimension = embedding_dimension \n",
    "        self.num_heads = num_heads\n",
    "        self.mlp_size = mlp_size\n",
    "        \n",
    "        self.msa_block = Msa(embedding_dimension = embedding_dimension , num_heads = num_heads)\n",
    "        self.mlp_block = MLPBlock(embedding_dimension = embedding_dimension , mlp_size = mlp_size)\n",
    "    def forward(self , x):\n",
    "        #Create Residual Connection (input = output + input of previous layer)\n",
    "        x = self.msa_block(x) + x\n",
    "        x = self.mlp_block(x) + x\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "82e366aa-36c4-4783-be7c-6890c72a8cfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Assembling ViT\n",
    "class Vision_transformer(nn.Module):\n",
    "    def __init__(self , patch_size = 16 , embedding_dimension = 768 , num_heads = 12 ,\n",
    "                mlp_size = 3072 , num_layers = 12 , img_size = 224 , num_classes = 1000):\n",
    "        super().__init__()\n",
    "        \n",
    "        num_patches = int((img_size*img_size)//patch_size**2)\n",
    "\n",
    "        self.class_embedding = nn.Parameter(torch.randn(1,1,embedding_dimension),requires_grad = True)\n",
    "        self.position_embedding = nn.Parameter(torch.randn(1 , num_patches+1 , embedding_dimension),requires_grad = True)\n",
    "        self.embedding_dropout = nn.Dropout(p = 0.1)\n",
    "        self.patch_embedding = PatchEmbedding(patch_size = patch_size , in_channels = 3 , out_channels = embedding_dimension)\n",
    "        self.transformer_encoder = nn.Sequential(*[TransformerEncoder(embedding_dimension = embedding_dimension,\n",
    "                                                                     num_heads = num_heads,\n",
    "                                                                     mlp_size = mlp_size) for i in range(num_layers)])\n",
    "        #MLP head\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.LayerNorm(normalized_shape = embedding_dimension),\n",
    "            nn.Linear(in_features = embedding_dimension , out_features = num_classes)\n",
    "        )\n",
    "    def forward(self , x):\n",
    "        batch_size = x.shape[0]\n",
    "        class_token = self.class_embedding.expand(batch_size , -1 ,-1)\n",
    "        x = self.patch_embedding(x)\n",
    "        #Concat class token with patch embedding (equation 1)\n",
    "        x = torch.cat((class_token , x) , dim = 1)\n",
    "        #Add position embedding with class token to the beginning (equation 1)\n",
    "        x = self.position_embedding+x\n",
    "        #Run embedding Dropout (Appendix B.1)\n",
    "        x = self.embedding_dropout(x)\n",
    "        #Equation 2 , 3\n",
    "        x = self.transformer_encoder(x)\n",
    "        #Equation 4\n",
    "        x = self.classifier(x[:,0])\n",
    "        return x"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
